We investigate in-context learning of classification preferences using the cooperative inverse reinforcement learning framework and image embedding models (CLIP) fine-tuned with a single-layer transformer and reinforcement learning from human feedback (RLHF). 

We set up an oracle model to generate class probabilities for a binary classification problem, generating synthetic training data, and then exploring in-context learning of preferences represented using decision thresholds. We frame in-context learning as implicit Bayesian inference, building on previous work on explicit Bayesian inference in the CIRL game between a human and a machine. Overall, our project aims to gain a better understanding of in-context learning of classification models using CLIP and RLHF, and to identify promising directions for future research in this area.



